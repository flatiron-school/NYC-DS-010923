{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-family:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Decision Trees\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC Feb 2023\n",
    "<p>Phase 3: Topic 26</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Decision Trees at a High Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Decision trees can be viewed as a series of forks in the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<a title=\"Jonathan Billinger / Fork in the road\" href=\"https://commons.wikimedia.org/wiki/File:Fork_in_the_road_-_geograph.org.uk_-_1355424.jpg\"><img width=\"512\" alt=\"Fork in the road - geograph.org.uk - 1355424\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/71/Fork_in_the_road_-_geograph.org.uk_-_1355424.jpg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Every time we make a decision, we split up, or *partition*, the data based on the features.\n",
    "\n",
    "- optimizing for a metric that aids us in separating feature space according to class assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Simple Example of a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say we have this set of data:\n",
    "\n",
    "Work Status |  Age  | Favorite Website\n",
    "------------|-------|-------------------------\n",
    " Student    | Young | A\n",
    " Working    | Young | B\n",
    " Working    | Old   | C\n",
    " Working    | Young | B\n",
    " Student    | Young | A\n",
    " Student    | Young | A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This can help us answer the question:\n",
    "\n",
    "Based on age and work status, what website will user likely choose as their favorite?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Picturing Decisions as a Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reasonable decision tree that might represent the data we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Work Status |  Age  | Favorite Website\n",
    "------------|-------|-------------------------\n",
    " Student    | Young | A\n",
    " Working    | Young | B\n",
    " Working    | Old   | C\n",
    " Working    | Young | B\n",
    " Student    | Young | A\n",
    " Student    | Young | A\n",
    " Student    | Old   | C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/simple_decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our first split was on Age: \n",
    "- chose based on observation: old people choose website C\n",
    "- leaf has only members belong to class C \n",
    "\n",
    "**This defines region of feature space where we predict class C**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![](images/simple_decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our first split was on Age: \n",
    "- inside feature space partition where Age = Young\n",
    "    - Notice that we have a mixed bag here\n",
    "    \n",
    "Work Status |  Age  | Favorite Website\n",
    "------------|-------|-------------------------\n",
    " Student    | Young | A\n",
    " Working    | Young | B\n",
    " Working    | Old   | C\n",
    " Working    | Young | B\n",
    " Student    | Young | A\n",
    " Student    | Young | A\n",
    " Student    | Old   | C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Use other feature (work status):\n",
    "- will it help us in determining between class A and B?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A further split based on work status inside current partition: differentiates between class A and class B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src = \"images/simple_decision_tree.png\" width = 400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Overview of Algorithm's Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Split data features $X$ and target $y$\n",
    "\n",
    "One big advantage of tree-based algorithms: \n",
    "- $X$ does not **need** to be scaled and often no transformations required\n",
    "- categorical feature can be handled easily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. Make a *decision* (a split) based on some *metric* using the features\n",
    "  - *Data split into partitions via *branches*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src = \"images/simple_decision_tree.png\" width = 400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3.Continue on with each partition, and do more splits for each using the features in that partition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<center><img src = \"images/simple_decision_tree.png\" width = 400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note: **Splitting is a recursive process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"images/recursion.png\" width = 400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Second level splits:\n",
    "- only conidering class differentiation criterion within each partition locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This sort of algorithm locally optimizing on specific criterion:\n",
    "- known as **greedy algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Greedy algorithms that work in recursive structure: often blindingly fast**\n",
    "\n",
    "- Yes, decision trees are really fast algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "4. Keep at this game: until a **stopping condition** is hit\n",
    "    - each partition(leaf) has only one class in it\n",
    "    - OR hit a pre-defined maximum tree depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<center><img src = \"images/decisiontreepure.gif\" width = 700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "5. To make predictions, flow data points $X$ down the tree: \n",
    "- through the decision nodes to predictions at the leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<center><img src = \"images/simple_decision_tree.png\" width = 400></center>\n",
    "<center>Tree is trained model.</center>\n",
    "\n",
    "We have new observation of a person: \n",
    "\n",
    "X = (Young, Working)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A very nice demo on decision trees using some real data:\n",
    "\n",
    "http://www.r2d3.us/visual-intro-to-machine-learning-part-1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Entropy/Gini Impurity and the Notion of Information Gain\n",
    "- Metrics used to decided where a split is made.\n",
    "- measures notions of class purity/heterogeneity in a given partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ideal goal: partitions are fully pure/homogenous with respect to class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<center><img src = \"images/decisiontreepure.gif\" width = 700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Entropy\n",
    "- A measure of class heterogeity within a partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The entropy within a partition is given by:\n",
    "\n",
    "$$ E = -\\sum_{i=1}^k p_i\\log_k(p_i)$$\n",
    "\n",
    "- $k$ is number of classes\n",
    "- $p_i$ is relative frequency of $i$th class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Take simple example of binary classification:\n",
    "\n",
    "$$ E = -\\sum_{i=1}^2 p_i\\log_2(p_i) = \\\\ -p_1 \\log_2p_1 - (1 - p_1)\\log_2(1-p_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"images/entropy_levels.png\" width = 700></center>\n",
    "<center>Left to right: low to high entropy, increasing heterogeneity</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Calculating the entropy for equal class support in partition**\n",
    "- high heterogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"images/entropy_equalpart.png\" width = 200></center>\n",
    "<center>Maximum heterogeneity</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since two groups and equal split between classes:\n",
    "\n",
    "- $ p_i = \\frac{1}{2}$ for  $i = 1,2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Calculating the entropy: $ E = -\\sum_{i=1}^2 p_i\\log_2(p_i) = -p_1 \\log_2p_1 - (1 - p_1)\\log_2(1-p_1)$\n",
    "\n",
    "$$ E =  -2*\\frac{1}{2}\\log_2(\\frac{1}{2}) $$\n",
    "$$E = -\\log_2(\\frac{1}{2})= 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Calculating the entropy for partition with only one class**\n",
    "- homogenous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$ E = -p_1 \\log_2p_1 - (1 - p_1)\\log_2p_1$\n",
    "\n",
    "where $p_1 = 1$\n",
    "\n",
    "$$ E = -\\log_2(1) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Looking at the ranges between:\n",
    "- entropy traces out a curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-7 #prevent explosion from log\n",
    "entropy = lambda p: -(p*np.log2(p + epsilon) \\\n",
    "                      + (1-p)*np.log(1-p + epsilon))\n",
    "\n",
    "p_range = np.linspace(0,1,50)\n",
    "entropy_array = entropy(p_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(p_range, entropy_array)\n",
    "plt.ylabel('Entropy', size = 15)\n",
    "plt.xlabel(r'$p_1$', size = 15)\n",
    "plt.title('Entropy for partition with two classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Entropy will always be between 0 and 1. The closer to 1, the more disordered your group.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For a given parent partition: want a split where\n",
    "- weighted average of entropy over children partition lower than parent partition entropy\n",
    "\n",
    "i.e., children are statistically speaking more pure than parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"images/partinfo.png\" width = 500></center>\n",
    "<center>Is the weighted sum of children entropy lower than parent?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Information Gain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**information gain** = **children entropy** - **parent's entropy** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What we want:\n",
    "\n",
    "A split on a feature $X_1$ that results in good information gain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"images/x1splitfeature.png\" width = 500></center>\n",
    "<center>Tuning $X_1$ for split: scanning feature space</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"images/x1feature_infogain.png\" width = 500></center>\n",
    "<center>Tuning $X_1$ for split: best information gain</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- choose value of $X_1$ to split on that has the lowest **children entropy** - **parent's entropy** (best information gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Decision trees optimization in partition:\n",
    "- computes information gain for many values of $X_1$\n",
    "- finds $X_1$ at minimum value of information gain\n",
    "- splits there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Gini Impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An alternative metric to entropy.\n",
    "\n",
    "Gini Impurity is defined as:\n",
    "\n",
    "$$ G = 1 - \\sum_{i=1}^k p_i^2 $$\n",
    "\n",
    "- Gini impurity heavily penalizes heterogeneity (more strongly than entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**For binary problem:**\n",
    "- $0 \\leq G \\leq 0.5$\n",
    "- Closer to 0.5, the more heterogenous the partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Calculation of information gain same: just with Gini impurity instead of entropy\n",
    "\n",
    "$$G_{children} - G_{parent}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Putting it all together**\n",
    "\n",
    "1. Choose feature $X$ and tune to get optimal split for information gain\n",
    "2. Make split and partition.\n",
    "3. In each new partition, choose a different $X'$.\n",
    "4. Tune to get optimal split for information gain.\n",
    "5. Make split and partition.\n",
    "6. Rinse, repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question**: Are we guaranteed, proceeding in this way, to reach pure groups, no matter what our data looks like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Decision trees very prone to overfitting.\n",
    "- will continue to split until all leafs are pure\n",
    "- can get way too complex (every point has its own partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Will need a way to limit tree complexity/depth to aid in generalization:\n",
    "- avoid overfitting\n",
    "\n",
    "**Strategies/decision tree hyperparameters that aid with this. Discuss these while tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Implement a decision tree classifier in Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Setting up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "iris_df = sns.load_dataset('iris')[['petal_width', 'sepal_length', 'species']]\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "iris_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = iris_df.drop(columns = ['species'])\n",
    "labenc = LabelEncoder()\n",
    "y = labenc.fit_transform(iris_df.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(iris_df, hue = 'species');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=13)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Training the model out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(criterion = 'gini', random_state=42)\n",
    "\n",
    "tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_tree(tree_clf, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "A complex model:\n",
    "- depth = 8\n",
    "- alarm bells should be going off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Train accuracy is pretty high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tree_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = tree_clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred) * 100\n",
    "print(\"Accuracy: {0}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#plot_confusion_matrix(tree_clf, X_test, y_test);\n",
    "ConfusionMatrixDisplay.from_estimator(tree_clf, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lower test set accuracy. Sign of variance issues. But how bad is it?\n",
    "\n",
    "- Visualizing decision boundary may help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_2D = X_train.values\n",
    "\n",
    "x_min, x_max = X_2D[:, 0].min() - 1, X_2D[:, 0].max() + 1\n",
    "y_min, y_max = X_2D[:, 1].min() - 1, X_2D[:, 1].max() + 1\n",
    "\n",
    "xx_n, yy_n = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, ax = plt.subplots(figsize = (10,5))\n",
    "\n",
    "Z = tree_clf.predict(np.c_[xx_n.ravel(), yy_n.ravel()])\n",
    "Z = Z.reshape(xx_n.shape)\n",
    "ax.contourf(xx_n, yy_n, Z, alpha=0.4)\n",
    "ax.scatter(X_2D[:, 0], X_2D[:, 1], c = y_train, s=30, edgecolor=\"k\")\n",
    "ax.set_xlabel('petal width [cm]')\n",
    "ax.set_ylabel('sepal length [cm]')\n",
    "ax.set_title('Decision Boundary: Decision Tree')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Boundary unstable to single points:\n",
    "- strange fluctuations in decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tree has split to high depth:\n",
    "- to get as pure as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bias-Variance with Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- CART algorithm repeatedly partition data into smaller and smaller subsets \n",
    "- until final subsets homogeneous in target. \n",
    "- often means final partitions (leaves of the tree): few data points. \n",
    "\n",
    "**Low-bias, high variance models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Need parameters that have a regularizing effect:\n",
    "- stopping criterion\n",
    "\n",
    "Put in some conditions to prevent tree growing to overfit train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Commonly tuned hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Max depth**\n",
    "-  DecisionTreeClassifier(max_depth = __)\n",
    "- Sets maximum depth tree can grow. Prevents overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"images/treedepth.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Stops tree from growing too big:\n",
    "- In actuality, grows tree all the way and uses rule-based algorithm to prune back to specified depth\n",
    "- Good values of max_depth depend on data complexity (can range from 2 to 12...sometimes bigger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Minimum number of samples in parent for allowing a split**\n",
    "-  DecisionTreeClassifier(min_samples_split = __)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src = \"images/minsampleplit.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Minimum samples in leaf allowed in children**\n",
    "\n",
    "- constrains number of samples in children when optimizing information gain on leafs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src = \"images/minsampleleaf.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Stop it from running too long\n",
    "tree_clf = DecisionTreeClassifier(criterion = 'entropy', max_depth = 3,  random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy on training data & test data\n",
    "print('Training:', tree_clf.score(X_train, y_train))\n",
    "print('Testing:', tree_clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_tree(tree_clf, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The fitted tree has an attribute called `ct.feature_importances_`:\n",
    "\n",
    "- Importance of feature: impurity decrease averaged over nodes splitting on given feature. (roughly) \n",
    "\n",
    "i.e., how useful is feature in model for aiding in class differentiation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "feature_used = X_train.columns\n",
    "dt.fit(X, y)\n",
    "\n",
    "for fi, feature in zip(dt.feature_importances_, feature_used):\n",
    "    print(fi, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The decision tree is a \"white-box\" type of ML algorithm. It shares internal decision-making logic, which is not available in the black-box type of algorithms such as Neural Network.\n",
    "- Its training time is faster compared to other algorithms such as neural networks.\n",
    "- The decision tree is a non-parametric method, which does not depend upon probability distribution assumptions.\n",
    "- Decision trees can handle high-dimensional data with good accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Pros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Easy to interpret and visualize model structure\n",
    "- Can easily capture non-linear patterns in features\n",
    "- Require little data preprocessing from the user (no need to normalize data)\n",
    "- Make no assumptions about distribution because its non-parametric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Sensitive to noisy data (overfit)\n",
    "- Trouble with imbalanced datasets\n",
    "- Predictive accuracy usually not as good as other approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But aggregating many decision trees together: \n",
    "- the predictive performance can be improved substantially.\n",
    "- still keep a high degree of speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Important Terminology Related to Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Root Node:** Represents entire population or sample.\n",
    "- **Decision Node:** Node that is split.\n",
    "- **Leaf/ Terminal Node:** Node with no children.\n",
    "- **Pruning:** Removing nodes.\n",
    "- **Branch / Sub-Tree:** A sub-section of a decision tree.\n",
    "- **Parent and Child Node:** A node divided into sub-nodes is the parent; the sub-nodes are its children.\n",
    "\n",
    "<img src='./images/decision_leaf.webp' width=600 />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dplearn",
   "language": "python",
   "name": "dplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "280.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
